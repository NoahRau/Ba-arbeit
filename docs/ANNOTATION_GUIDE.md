# Anleitung zur manuellen Annotation

## ğŸ“‹ Was wurde generiert?

**Datei:** `hybrid_pipeline_matches.csv`

Die Hybrid Pipeline hat **alle 56 S1000D-Konzepte** verarbeitet und folgende Ergebnisse erzielt:

- âœ… **9 Matches gefunden** (16.1%)
- âŒ **47 No Match / NULL** (83.9%)

Die Pipeline ist sehr konservativ (hohe PrÃ¤zision), daher sind viele Matches als NULL markiert.

---

## ğŸ“ Ihre Aufgabe: Manuelle Annotation

### Schritt 1: CSV-Datei Ã¶ffnen

Ã–ffnen Sie `hybrid_pipeline_matches.csv` in Excel oder Google Sheets.

### Schritt 2: Spalten verstehen

**Wichtige Spalten:**

| Spalte | Beschreibung |
|--------|--------------|
| `s1000d_label` | S1000D Konzept (z.B. "Bicycle - Description") |
| `s1000d_context` | Hierarchischer Kontext des S1000D-Konzepts |
| `pipeline_selected_uri` | Von der Pipeline gewÃ¤hlter Match (oder "NULL") |
| `selected_label` | Label des gewÃ¤hlten Matches |
| `pipeline_confidence` | Confidence-Score der Pipeline (0-1) |
| `candidate_1_uri` bis `candidate_5_uri` | Top-5 alternative Kandidaten |
| `candidate_1_label` bis `candidate_5_label` | Labels der Kandidaten |
| `candidate_1_score` bis `candidate_5_score` | Aggregierte Scores |
| **`is_match_manual`** | â† **HIER EINTRAGEN!** |
| `correct_match_uri` | â† Optional: Korrekter Match wenn FALSE |
| `notes` | â† Optional: Ihre Notizen |

### Schritt 3: Annotation durchfÃ¼hren

FÃ¼r **jede Zeile** (56 insgesamt):

#### Fall 1: Pipeline hat Match gefunden (`pipeline_selected_uri` â‰  "NULL")

**Frage:** Ist `selected_label` korrekt fÃ¼r `s1000d_label`?

- âœ… **Ja, korrekt** â†’ Eintragen: `TRUE`
- âŒ **Nein, falsch** â†’ Eintragen: `FALSE`
  - Optional: In `correct_match_uri` den richtigen URI aus `candidate_1_uri` bis `candidate_5_uri` eintragen
  - Optional: In `notes` BegrÃ¼ndung schreiben

**Beispiel:**
```
s1000d_label: "Bicycle - Description of how it is made"
selected_label: "Bike"
â†’ Passt "Bike" zu "Bicycle"? â†’ JA â†’ is_match_manual = TRUE
```

#### Fall 2: Pipeline hat KEINEN Match gefunden (`pipeline_selected_uri` = "NULL")

**Frage:** Sollte es einen Match geben?

- âœ… **Ja, es sollte einen Match geben** â†’ Eintragen: `FALSE` (Pipeline hat Fehler gemacht)
  - Optional: In `correct_match_uri` den richtigen URI aus `candidate_1_uri` bis `candidate_5_uri` eintragen
  - Optional: In `notes` schreiben "Should match candidate X"

- âŒ **Nein, NULL ist korrekt** â†’ Eintragen: `TRUE` (Pipeline hatte Recht)

**Beispiel:**
```
s1000d_label: "Mountain bicycle - Business rules"
selected_label: (leer, weil NULL)
â†’ Gibt es ein passendes Konzept in den Kandidaten? â†’ NEIN â†’ is_match_manual = TRUE (NULL ist korrekt)
```

**Beispiel 2:**
```
s1000d_label: "Wheel - Description"
selected_label: (leer, weil NULL)
candidate_1_label: "Wheel"
â†’ "Wheel" passt perfekt! â†’ is_match_manual = FALSE (Pipeline hÃ¤tte matchen sollen)
â†’ correct_match_uri = http://purl.org/ontology/bikeo#Wheel
```

---

## âœï¸ Annotation-Regeln

### Was ist ein MATCH?

âœ… **Match = TRUE**, wenn:
- Beide Konzepte **exakt dasselbe** reprÃ¤sentieren
- Funktionale Ã„quivalenz gegeben ist
- Beispiele:
  - "Bicycle" â†” "Bike" âœ“
  - "Wheel" â†” "Wheel" âœ“
  - "Brake System - Description" â†” "Brake" âœ“ (wenn Kontext passt)

âŒ **Match = FALSE**, wenn:
- Nur verwandt, aber nicht identisch
- Parent-Child Beziehung (z.B. "Wheel" â‰  "Hub")
- Geschwister (z.B. "Front Brake" â‰  "Rear Brake")
- Verschiedene Aspekte (z.B. "Maintenance Procedure" â‰  "Description")

### Bei Unsicherheit

- Schauen Sie sich **alle 5 Kandidaten** an
- Lesen Sie den **Kontext** (`s1000d_context`)
- Im Zweifel: Konservativ sein (lieber FALSE)

---

## ğŸ’¾ Speichern

Nach der Annotation:
1. Speichern Sie die Datei als `hybrid_pipeline_matches_ANNOTATED.csv`
2. Stellen Sie sicher, dass die Spalte `is_match_manual` fÃ¼r **alle 56 Zeilen** ausgefÃ¼llt ist

---

## ğŸ”¬ Evaluation

Nach der Annotation wird das Evaluation-Script ausgefÃ¼hrt:

```bash
python evaluate_annotated_matches.py hybrid_pipeline_matches_ANNOTATED.csv --plot
```

Das generiert:
- **Evaluation Report** (Markdown mit MCC, F1, Precision, Recall)
- **Confusion Matrix** (Visualisierung)
- **Error Analysis** (Welche Fehler hat die Pipeline gemacht?)

---

## ğŸ“Š Beispiel-Zeilen

### Beispiel 1: Pipeline korrekt (Match gefunden)
```csv
s1000d_label: "Bicycle - Description of function"
selected_label: "Bike"
pipeline_confidence: 0.95
â†’ is_match_manual: TRUE  â† Korrekt!
```

### Beispiel 2: Pipeline korrekt (NULL)
```csv
s1000d_label: "Mountain bicycle - Business rules"
selected_label: (NULL)
â†’ is_match_manual: TRUE  â† Korrekt, es gibt kein passendes Konzept
```

### Beispiel 3: False Positive
```csv
s1000d_label: "Lighting - Maintenance"
selected_label: "Bike"
â†’ is_match_manual: FALSE  â† Falsch! "Bike" passt nicht zu "Lighting"
â†’ notes: "Should be NULL or Light-related concept"
```

### Beispiel 4: False Negative
```csv
s1000d_label: "Wheel - Description"
selected_label: (NULL)
candidate_1_label: "Wheel"
â†’ is_match_manual: FALSE  â† Pipeline hat Fehler gemacht
â†’ correct_match_uri: http://purl.org/ontology/bikeo#Wheel
â†’ notes: "Candidate 1 is perfect match"
```

---

## â±ï¸ Zeitaufwand

- **GeschÃ¤tzte Dauer:** 20-30 Minuten fÃ¼r 56 Konzepte
- **Pro Zeile:** ~30 Sekunden

---

## â“ Fragen?

Bei Unklarheiten:
1. Schauen Sie sich die Top-5 Kandidaten an
2. Lesen Sie den Kontext
3. Nutzen Sie die `notes` Spalte fÃ¼r Unsicherheiten

**Viel Erfolg bei der Annotation!** ğŸ“
